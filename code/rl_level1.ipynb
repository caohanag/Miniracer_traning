{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Oct 12 14:29:55 2022\n",
    "\n",
    "@author: oliver\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import count\n",
    "import time\n",
    "\n",
    "from kyoka.task import BaseTask\n",
    "from kyoka.algorithm.sarsa import SarsaTabularActionValueFunction\n",
    "from kyoka.callback import BasePerformanceWatcher\n",
    "from kyoka.algorithm.sarsa import Sarsa\n",
    "from kyoka.policy import EpsilonGreedyPolicy\n",
    "from kyoka.callback import BaseCallback\n",
    "from kyoka.policy import choose_best_action\n",
    "\n",
    "\n",
    "class Minirace(BaseCallback):\n",
    "\n",
    "    def __init__(self, level=1, size=6, normalise=False):\n",
    "        # level is the dimensionality of state vector (1 or 2)\n",
    "        self.level = min(2, max(1, level))  \n",
    "        # size is the number of track positions (every 2 pixels, from 1..n-1).\n",
    "        # this means there are 2 more car positions (0 and n)s\n",
    "        self.size = max(3, size)\n",
    "        self.xymax = 2 * (self.size + 2)  # 16\n",
    "        # whether to normalise the state representation\n",
    "        self.scale = 2 if normalise and level > 1 else 1.0\n",
    "\n",
    "        # the internal state is s1 = (x, z, d). Previous internal state in s0.\n",
    "        # x: x-coordinate of the car\n",
    "        # z[]: x-coordinate of the track, one for each y-coordinate\n",
    "        # d[]: dx for the track, for each y-coordinate(dx:the relative position of the middle of the track right in front of the car)\n",
    "        self.generate_initial_state()\n",
    "\n",
    "    def observationspace(self):\n",
    "        \"\"\"\n",
    "        Dimensionality of the observation space\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of values return as an observation.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.level\n",
    "\n",
    "    def nexttrack(self, z, d=2):\n",
    "        \"\"\"\n",
    "        Move the next piece of track based on coordinate z, and curvature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : int\n",
    "            x-coordinate for the previous track segment.\n",
    "        d : int, optional\n",
    "            previous \"curvature\" (change of coordinate). The default is 2.\n",
    "            This is to prevent too strong curvature (car can only move 1 step)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        znext : int\n",
    "            The x-coordinate for the next piece of track (middle of the track).\n",
    "        dz : int\n",
    "            The change compared to the previous coordinate (-2..2).\n",
    "\n",
    "        \"\"\"\n",
    "        trackd = random.randint(-2, 2)\n",
    "        if self.level == 1 or abs(d) > 1:\n",
    "            trackd = min(1, max(-1, trackd))  \n",
    "\n",
    "        znext = max(1, min(self.size, z + trackd)) \n",
    "        dz = znext - z\n",
    "        return znext, dz\n",
    "\n",
    "    def state(self):\n",
    "        \"\"\"\n",
    "        Returns the (observed) state of the system.\n",
    "\n",
    "        Depending on level, the observed state is an\n",
    "        array of 1 to 5 values, or a pixel representation (level 0).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            level 1: [dx]\n",
    "                 dx: relative distance in x-coordinate between car and next\n",
    "                     piece of track (the one in front of the car).\n",
    "                     May be normalised to values between -1 and 1,\n",
    "                     depending on initialisation.\n",
    "        \"\"\"\n",
    "        x, z, d = self.s1\n",
    "        # print(\"self.s1:\", self.s1)\n",
    "        ## level 1:\n",
    "        # return the difference between car x and the next piece of track\n",
    "        if self.level == 1:\n",
    "            return np.array([(z[2] - x) / self.scale])\n",
    "        if self.level == 2:\n",
    "            return np.array([(z[2] - x) / self.scale, (z[3] - z[2]) / self.scale])\n",
    "\n",
    "        raise ValueError(\"level not implemented\")\n",
    "\n",
    "    def transit_state(self, state, action=0):\n",
    "        \"\"\"\n",
    "        Apply an action and update the environment.\n",
    "        0: do nothing\n",
    "        1: move left\n",
    "        2: move right\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int, optional\n",
    "            The action applied before the update.\n",
    "            The default is 0 (representing no action).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            The new observed state of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        self.s0 = self.s1\n",
    "\n",
    "        # state = self.state()\n",
    "        if self.is_terminal_state(state):\n",
    "            return self.state()\n",
    "\n",
    "        x0, z0, d0 = self.s0\n",
    "        z1 = np.roll(z0, -1)  \n",
    "        d1 = np.roll(d0, -1)\n",
    "\n",
    "        x1 = x0\n",
    "        if action == 1:\n",
    "            x1 = max(0, x0 - 1)\n",
    "        elif action == 2:\n",
    "            x1 = min(self.size - 1, x0 + 1)  # 0<= x <=5\n",
    "\n",
    "        z1[-1], d1[-1] = self.nexttrack(z0[-1], d0[-1])\n",
    "        self.s1 = (x1, z1, d1)\n",
    "        return self.state()\n",
    "\n",
    "    def generate_possible_actions(self, state):\n",
    "        return [0, 1, 2]\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"\n",
    "        Check if episode is finished.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if episode is finished.\n",
    "\n",
    "        \"\"\"\n",
    "        x, z, _ = self.s1\n",
    "\n",
    "        return abs(z[1] - x) > 1.0  # 0 +1 -1 which mean [car in middle of the track, car in left of the track, car in right of the track]\n",
    "\n",
    "    def calculate_reward(self, state):\n",
    "        \"\"\"\n",
    "        Calculate immediate reward.\n",
    "        Positive reward for staying on track.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            0-2, for the 3 possible actions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        r : float\n",
    "            immediate reward.\n",
    "\n",
    "        \"\"\"\n",
    "        r = 1.0 if not self.is_terminal_state(state) else 0.0\n",
    "\n",
    "        return r\n",
    "\n",
    "    def step(self, action):\n",
    "        # return tuple (state, reward, done)\n",
    "        cur_state = self.state()\n",
    "        state = self.transit_state(cur_state, action)\n",
    "        r = self.calculate_reward(state)\n",
    "        done = self.is_terminal_state(state)\n",
    "        return (state, r, done)\n",
    "\n",
    "    def generate_initial_state(self):\n",
    "        # the internal state is\n",
    "        # x: x-coordinate of the car  \n",
    "        # z[]: x-coordinate of the track, one for each y-coordinate  \n",
    "        # d[]: dx for the track, for each y-coordinate   \n",
    "        x = random.randint(0, self.size - 1)  # [0, self.size-1]\n",
    "        z = np.zeros(self.xymax)\n",
    "        d = np.zeros(self.xymax)\n",
    "        z[0] = max(1, min(self.size, x))\n",
    "        z[1] = z[0]  \n",
    "        d[1] = 2  \n",
    "        for i in range(2, self.xymax):\n",
    "            z[i], d[i] = self.nexttrack(z[i - 1], d[i - 1])\n",
    "\n",
    "        self.s0 = (x, z, d)\n",
    "        self.s1 = (x, z, d)\n",
    "        '''\n",
    "        print(\"self.s0:\", self.s0)\n",
    "        print(\"self.s1:\", self.s1)\n",
    "        self.render()\n",
    "        '''\n",
    "        return self.state()\n",
    "\n",
    "    def sampleaction(self):  # random strategy\n",
    "        # return a random action [0,2]\n",
    "        action = random.randint(0, 2)\n",
    "        return action\n",
    "\n",
    "    def render(self, text=True, reward=None, cm=plt.cm.bone_r, f=None):  \n",
    "        x, z, _ = self.s1\n",
    "        pix = self.to_pix(x, z, text)\n",
    "        if text:\n",
    "            if reward is not None:\n",
    "                print('{:.3f}'.format(reward))\n",
    "            print(''.join(np.flip(pix, axis=0).ravel()))\n",
    "        else:\n",
    "            fig, ax = plt.subplots()\n",
    "            if reward is not None:\n",
    "                plt.title(f'Reward: {reward}', loc='right')\n",
    "            ax.axis(\"off\")\n",
    "            plt.imshow(pix, origin='lower', cmap=cm)\n",
    "            if f is not None:\n",
    "                plt.savefig(f, dpi=300)\n",
    "            plt.show()\n",
    "\n",
    "    def to_pix(self, x, z, text=False):  \n",
    "        \"\"\"\n",
    "        Generate a picture from an internal state representation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int\n",
    "            car x-coordinate\n",
    "        z : np.array\n",
    "            array with track coordinates\n",
    "        text : bool, optional\n",
    "            flag if generate text represenation\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If x,y,z are outside their range.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : np.array\n",
    "            a square image with pixel values 0, 0.5, and 1.\n",
    "\n",
    "        \"\"\"\n",
    "        if x < 0 or x > self.size + 1:\n",
    "            raise ValueError('car coordinate value error')\n",
    "        if np.min(z) < 1 or np.max(z) > self.size:\n",
    "            raise ValueError('track coordinate value error')\n",
    "\n",
    "        car = '#' if text else 2\n",
    "\n",
    "        if text:\n",
    "            image = np.array(list(':' * (self.xymax + 1)) * (self.xymax)).reshape(self.xymax, -1)\n",
    "            image[:, -1] = '\\n'\n",
    "        else:\n",
    "            image = np.ones((self.xymax, self.xymax), dtype=int)\n",
    "\n",
    "        for i, j in enumerate(z):\n",
    "            j = int(j * 2)\n",
    "            image[i, j - 2:j + 4] = ' ' if text else 0\n",
    "\n",
    "        image[0:2, 2 * x:(2 * x + 2)] = car\n",
    "\n",
    "        return image\n",
    "\n",
    "    def state_based_action(self, state):\n",
    "        if self.level == 1:  # no normalization\n",
    "            if state >= 1:\n",
    "                action = 2  # car turn right\n",
    "            elif state == 0:\n",
    "                action = 0\n",
    "            elif state <= -1:\n",
    "                action = 1  # car turn left\n",
    "        return action\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MazeTabularValueFunction(SarsaTabularActionValueFunction):\n",
    "\n",
    "    def generate_initial_table(self):\n",
    "        state_num, action_num = 13, 3\n",
    "        table = [[0 for a in range(action_num)] for j in range(state_num)]  # 13*4\n",
    "        '''\n",
    "        table = [[0, 1, -1], [0, 1, -1], [0, 1, -1], [0, 1, -1], [0, 1, -1], [0.5, 1, -1], [1, 0.5, 0.5], [0.5, -1, 1],\n",
    "                 [0, -1, 1], [0, -1, 1], [0, -1, 1], [0, -1, 1], [0, -1, 1]]\n",
    "                 '''\n",
    "        return table\n",
    "\n",
    "    def fetch_value_from_table(self, table, state, action):\n",
    "        # print(\"state:\", state)\n",
    "        return table[int(state) + 6][action]\n",
    "\n",
    "    def insert_value_into_table(self, table, state, action, new_value):\n",
    "        table[int(state) + 6][action] = new_value\n",
    "\n",
    "\n",
    "class MazeTransformationCallback(BaseCallback):\n",
    "\n",
    "    def after_update(self, iteration_count, therace, value_function):\n",
    "        if 50 == iteration_count:\n",
    "            therace.generate_initial_state()\n",
    "            # we recommend you to use \"self.log(message)\" instead of \"print\" method in callback.\n",
    "            self.log(\"Maze transformed after %d iteration.\" % iteration_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mypolicy_v0(state):\n",
    "    action = therace.sampleaction()\n",
    "    return action\n",
    "\n",
    "\n",
    "def mypolicy(state):\n",
    "    # selecting actions based on the state information\n",
    "    action = therace.state_based_action(state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action ValueTable(initial) [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "[Progress] Start GPI iteration for 100 times\n",
      "[EpsilonGreedyAnnealing] Anneal epsilon from 1 to 0.01.\n",
      "[Progress] Finished 1 / 100 iterations (0.0s)\n",
      "[EpsilonGreedyAnnealing] Annealing has finished at 1 iteration.\n",
      "[Progress] Finished 2 / 100 iterations (0.0s)\n",
      "[Progress] Finished 3 / 100 iterations (0.0s)\n",
      "[Progress] Finished 4 / 100 iterations (0.0s)\n",
      "[Progress] Finished 5 / 100 iterations (0.0s)\n",
      "[Progress] Finished 6 / 100 iterations (0.0s)\n",
      "[Progress] Finished 7 / 100 iterations (0.0s)\n",
      "[Progress] Finished 8 / 100 iterations (0.0s)\n",
      "[Progress] Finished 9 / 100 iterations (0.0s)\n",
      "[Progress] Finished 10 / 100 iterations (0.0s)\n",
      "[Progress] Finished 11 / 100 iterations (0.0s)\n",
      "[Progress] Finished 12 / 100 iterations (0.0s)\n",
      "[Progress] Finished 13 / 100 iterations (0.0s)\n",
      "[Progress] Finished 14 / 100 iterations (0.0s)\n",
      "[Progress] Finished 15 / 100 iterations (0.0s)\n",
      "[Progress] Finished 16 / 100 iterations (0.0s)\n",
      "[Progress] Finished 17 / 100 iterations (0.0s)\n",
      "[Progress] Finished 18 / 100 iterations (0.0s)\n",
      "[Progress] Finished 19 / 100 iterations (0.0s)\n",
      "[Progress] Finished 20 / 100 iterations (0.0s)\n",
      "[Progress] Finished 21 / 100 iterations (0.0s)\n",
      "[Progress] Finished 22 / 100 iterations (0.0s)\n",
      "[Progress] Finished 23 / 100 iterations (0.0s)\n",
      "[Progress] Finished 24 / 100 iterations (0.0s)\n",
      "[Progress] Finished 25 / 100 iterations (0.0s)\n",
      "[Progress] Finished 26 / 100 iterations (0.0s)\n",
      "[Progress] Finished 27 / 100 iterations (0.0s)\n",
      "[Progress] Finished 28 / 100 iterations (0.0s)\n",
      "[Progress] Finished 29 / 100 iterations (0.0s)\n",
      "[Progress] Finished 30 / 100 iterations (0.0s)\n",
      "[Progress] Finished 31 / 100 iterations (0.0s)\n",
      "[Progress] Finished 32 / 100 iterations (0.0s)\n",
      "[Progress] Finished 33 / 100 iterations (0.0s)\n",
      "[Progress] Finished 34 / 100 iterations (0.0s)\n",
      "[Progress] Finished 35 / 100 iterations (0.0s)\n",
      "[Progress] Finished 36 / 100 iterations (0.0s)\n",
      "[Progress] Finished 37 / 100 iterations (0.0s)\n",
      "[Progress] Finished 38 / 100 iterations (0.0s)\n",
      "[Progress] Finished 39 / 100 iterations (0.0s)\n",
      "[Progress] Finished 40 / 100 iterations (0.0s)\n",
      "[Progress] Finished 41 / 100 iterations (0.0s)\n",
      "[Progress] Finished 42 / 100 iterations (0.0s)\n",
      "[Progress] Finished 43 / 100 iterations (0.0s)\n",
      "[Progress] Finished 44 / 100 iterations (0.0s)\n",
      "[Progress] Finished 45 / 100 iterations (0.0s)\n",
      "[Progress] Finished 46 / 100 iterations (0.0s)\n",
      "[Progress] Finished 47 / 100 iterations (0.0s)\n",
      "[Progress] Finished 48 / 100 iterations (0.0s)\n",
      "[Progress] Finished 49 / 100 iterations (0.0s)\n",
      "[Progress] Finished 50 / 100 iterations (0.0s)\n",
      "[Progress] Finished 51 / 100 iterations (0.0s)\n",
      "[Progress] Finished 52 / 100 iterations (0.0s)\n",
      "[Progress] Finished 53 / 100 iterations (0.0s)\n",
      "[Progress] Finished 54 / 100 iterations (0.0s)\n",
      "[Progress] Finished 55 / 100 iterations (0.0s)\n",
      "[Progress] Finished 56 / 100 iterations (0.0s)\n",
      "[Progress] Finished 57 / 100 iterations (0.0s)\n",
      "[Progress] Finished 58 / 100 iterations (0.0s)\n",
      "[Progress] Finished 59 / 100 iterations (0.0s)\n",
      "[Progress] Finished 60 / 100 iterations (0.0s)\n",
      "[Progress] Finished 61 / 100 iterations (0.0s)\n",
      "[Progress] Finished 62 / 100 iterations (0.0s)\n",
      "[Progress] Finished 63 / 100 iterations (0.0s)\n",
      "[Progress] Finished 64 / 100 iterations (0.0s)\n",
      "[Progress] Finished 65 / 100 iterations (0.0s)\n",
      "[Progress] Finished 66 / 100 iterations (0.0s)\n",
      "[Progress] Finished 67 / 100 iterations (0.0s)\n",
      "[Progress] Finished 68 / 100 iterations (0.0s)\n",
      "[Progress] Finished 69 / 100 iterations (0.0s)\n",
      "[Progress] Finished 70 / 100 iterations (0.0s)\n",
      "[Progress] Finished 71 / 100 iterations (0.0s)\n",
      "[Progress] Finished 72 / 100 iterations (0.0s)\n",
      "[Progress] Finished 73 / 100 iterations (0.0s)\n",
      "[Progress] Finished 74 / 100 iterations (0.0s)\n",
      "[Progress] Finished 75 / 100 iterations (0.0s)\n",
      "[Progress] Finished 76 / 100 iterations (0.0s)\n",
      "[Progress] Finished 77 / 100 iterations (0.0s)\n",
      "[Progress] Finished 78 / 100 iterations (0.0s)\n",
      "[Progress] Finished 79 / 100 iterations (0.0s)\n",
      "[Progress] Finished 80 / 100 iterations (0.0s)\n",
      "[Progress] Finished 81 / 100 iterations (0.0s)\n",
      "[Progress] Finished 82 / 100 iterations (0.0s)\n",
      "[Progress] Finished 83 / 100 iterations (0.0s)\n",
      "[Progress] Finished 84 / 100 iterations (0.0s)\n",
      "[Progress] Finished 85 / 100 iterations (0.0s)\n",
      "[Progress] Finished 86 / 100 iterations (0.0s)\n",
      "[Progress] Finished 87 / 100 iterations (0.0s)\n",
      "[Progress] Finished 88 / 100 iterations (0.0s)\n",
      "[Progress] Finished 89 / 100 iterations (0.0s)\n",
      "[Progress] Finished 90 / 100 iterations (0.0s)\n",
      "[Progress] Finished 91 / 100 iterations (0.0s)\n",
      "[Progress] Finished 92 / 100 iterations (0.0s)\n",
      "[Progress] Finished 93 / 100 iterations (0.0s)\n",
      "[Progress] Finished 94 / 100 iterations (0.0s)\n",
      "[Progress] Finished 95 / 100 iterations (0.0s)\n",
      "[Progress] Finished 96 / 100 iterations (0.0s)\n",
      "[Progress] Finished 97 / 100 iterations (0.1s)\n",
      "[Progress] Finished 98 / 100 iterations (0.0s)\n",
      "[Progress] Finished 99 / 100 iterations (0.0s)\n",
      "[Progress] Finished 100 / 100 iterations (0.1s)\n",
      "[Progress] Completed GPI iteration for 100 times. (total time: 0s)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action ValueTable(final): [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0.0, 43.40089126040047, 0.0], [49.3706267451947, 6.0031920285251275, 0.0], [49.66251188412054, 2.1747550703153897, 11.20955889950081], [49.562313541449164, 0.25079573093430796, 9.99189317156236], [0.0, 0.0, 48.09937112921078], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "Episode 0\t Last reward: 10000.00\n",
      "Episode 1\t Last reward: 10000.00\n",
      "Episode 2\t Last reward: 10000.00\n",
      "Episode 3\t Last reward: 10000.00\n",
      "Episode 4\t Last reward: 10000.00\n",
      "Episode 5\t Last reward: 10000.00\n",
      "Episode 6\t Last reward: 10000.00\n",
      "Episode 7\t Last reward: 10000.00\n",
      "Episode 8\t Last reward: 10000.00\n",
      "Episode 9\t Last reward: 10000.00\n",
      "Episode 10\t Last reward: 10000.00\n",
      "Episode 11\t Last reward: 10000.00\n",
      "Episode 12\t Last reward: 10000.00\n",
      "Episode 13\t Last reward: 10000.00\n",
      "Episode 14\t Last reward: 10000.00\n",
      "Episode 15\t Last reward: 10000.00\n",
      "Episode 16\t Last reward: 10000.00\n",
      "Episode 17\t Last reward: 10000.00\n",
      "Episode 18\t Last reward: 10000.00\n",
      "Episode 19\t Last reward: 10000.00\n",
      "Episode 20\t Last reward: 10000.00\n",
      "Episode 21\t Last reward: 10000.00\n",
      "Episode 22\t Last reward: 10000.00\n",
      "Episode 23\t Last reward: 10000.00\n",
      "Episode 24\t Last reward: 10000.00\n",
      "Episode 25\t Last reward: 10000.00\n",
      "Episode 26\t Last reward: 10000.00\n",
      "Episode 27\t Last reward: 10000.00\n",
      "Episode 28\t Last reward: 10000.00\n",
      "Episode 29\t Last reward: 10000.00\n",
      "Episode 30\t Last reward: 10000.00\n",
      "Episode 31\t Last reward: 10000.00\n",
      "Episode 32\t Last reward: 10000.00\n",
      "Episode 33\t Last reward: 10000.00\n",
      "Episode 34\t Last reward: 10000.00\n",
      "Episode 35\t Last reward: 10000.00\n",
      "Episode 36\t Last reward: 10000.00\n",
      "Episode 37\t Last reward: 10000.00\n",
      "Episode 38\t Last reward: 10000.00\n",
      "Episode 39\t Last reward: 10000.00\n",
      "Episode 40\t Last reward: 10000.00\n",
      "Episode 41\t Last reward: 10000.00\n",
      "Episode 42\t Last reward: 10000.00\n",
      "Episode 43\t Last reward: 10000.00\n",
      "Episode 44\t Last reward: 10000.00\n",
      "Episode 45\t Last reward: 10000.00\n",
      "Episode 46\t Last reward: 10000.00\n",
      "Episode 47\t Last reward: 10000.00\n",
      "Episode 48\t Last reward: 10000.00\n",
      "Episode 49\t Last reward: 10000.00\n",
      "****************************Reinforcement learning strategies*************************\n",
      "test 500 episode max_step=10000 mean=10000.000000\n",
      "test 500 episode max_step==10000 standard_deviation=0.000000\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parameter\n",
    "    seed = 1\n",
    "    gamma = 0.99\n",
    "    render = True\n",
    "    finalrender = True\n",
    "    log_interval = 1  # 100\n",
    "    render_interval = 1  # 1000\n",
    "    running_reward = 0\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # train\n",
    "    therace = Minirace(level=1, size=6)\n",
    "    policy = EpsilonGreedyPolicy(eps=1)\n",
    "    policy.set_eps_annealing(initial_eps=1, final_eps=0.01, anneal_duration=0.01)\n",
    "    value_function = MazeTabularValueFunction()\n",
    "    algorithm = Sarsa(gamma=gamma) #by default: alpha = 0.1 gamma = 0.9\n",
    "    algorithm.setup(therace, policy, value_function)\n",
    "    #callbacks = [MazeTransformationCallback(), MazePerformanceWatcher()]\n",
    "\n",
    "    \n",
    "    # print initial State-Action ValueTable\n",
    "    actions = [0, 1, 2]\n",
    "    Q_value_for_actions = [[value_function.predict_value(state, action) for action in actions] for state in range(-6, 7)]\n",
    "    print(\"State-Action ValueTable(initial)\", Q_value_for_actions)\n",
    "\n",
    "    train_ep_reward = algorithm.run_gpi(nb_iteration=100)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # visualizatino\n",
    "    plt.ion()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_ep_reward)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Epoch Reward')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # print final State-Action ValueTable\n",
    "    actions = [0, 1, 2]\n",
    "    Q_value_for_actions = [[value_function.predict_value(state, action) for action in actions] for state in range(-6, 7)]\n",
    "    print(\"State-Action ValueTable(final):\", Q_value_for_actions)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Random strategies test \n",
    "    random_reward_list = []\n",
    "    for i_episode in range(50): \n",
    "        state, ep_reward, done = therace.generate_initial_state(), 0, False\n",
    "        for t in range(0,10000):  # Don't infinite loop while learning\n",
    "            #action = choose_best_action(therace, value_function, state)\n",
    "            action = mypolicy_v0(state)\n",
    "            state, reward, done = therace.step(action) \n",
    "            reward = float(reward)  # strange things happen if reward is an int\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        random_reward_list.append(ep_reward)\n",
    "        print('Episode {}\\t Last reward: {:.2f}'.format(i_episode, ep_reward))\n",
    "\n",
    "    a_mean = np.mean(random_reward_list)\n",
    "    a_std = np.std(random_reward_list, ddof=1)\n",
    "    print(\"****************************Random strategies*************************\")\n",
    "    print(\"test 500 episode max_step=10000 mean=%f\" % a_mean)\n",
    "    print(\"test 500 episode max_step==10000 standard_deviation=%f\" % a_std)\n",
    "    print(\"**********************************************************************\")\n",
    "    '''\n",
    "\n",
    "    # Reinforcement learning strategies test \n",
    "    reward_list = []\n",
    "    for i_episode in range(50): \n",
    "        state, ep_reward, done = therace.generate_initial_state(), 0, False\n",
    "        for t in range(0,10000):  # Don't infinite loop while learning\n",
    "            action = choose_best_action(therace, value_function, state)\n",
    "            #action = mypolicy(state)\n",
    "            state, reward, done = therace.step(action) \n",
    "            reward = float(reward)  # strange things happen if reward is an int\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_list.append(ep_reward)\n",
    "        print('Episode {}\\t Last reward: {:.2f}'.format(i_episode, ep_reward))\n",
    "\n",
    "    a_mean = np.mean(reward_list)\n",
    "    a_std = np.std(reward_list, ddof=1)\n",
    "    print(\"****************************Reinforcement learning strategies*************************\")\n",
    "    print(\"test 500 episode max_step=10000 mean=%f\" % a_mean)\n",
    "    print(\"test 500 episode max_step==10000 standard_deviation=%f\" % a_std)\n",
    "    print(\"**********************************************************************\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33622b258e849c48cdbc49d77946df62c6974755a1354989ad23976d61ac869e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
