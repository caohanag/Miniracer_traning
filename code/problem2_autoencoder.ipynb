{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SZZaV21xxu"
      },
      "source": [
        "Convolution Autoencoders\n",
        "======"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "WhRM7s4K1xx3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVwYZtQ91xx6"
      },
      "source": [
        "Parameter Settings\n",
        "-------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "bxNlZzje1xx7"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "capacity = 1\n",
        "learning_rate = 1e-2\n",
        "use_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUKIs7Ay1xx8"
      },
      "source": [
        "Data Loading\n",
        "-------------------\n",
        "\n",
        "Screenshots show in 16x16 images. We normalize them, which gives a slight performance boost during training.\n",
        "We create both a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Construct\n",
        "-------------------\n",
        "\n",
        "As problem 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, height=16, width=16, transforms=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.labels = np.asarray(self.data.iloc[:, 256])\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # 1D array ([256]) reshape -> 2D array ([16,16])\n",
        "        img_as_np = np.asarray(self.data.iloc[index][0:256]).reshape(16, 16, 1).astype(float)\n",
        "\n",
        "        #print(\"img_as_np:\",img_as_np.shape[0],img_as_np.shape[1],img_as_np.shape[2])\n",
        "        \n",
        "        # transform numpy to tensor\n",
        "        if self.transforms is not None:\n",
        "            img_as_tensor = self.transforms(img_as_np)\n",
        "            img_as_tensor = img_as_tensor.type(torch.FloatTensor)\n",
        "        #print(img_as_tensor.type())\n",
        "        #print(\"img_as_tensor:\",img_as_tensor.shape[0],img_as_tensor.shape[1],img_as_tensor.shape[2])\n",
        "        return (img_as_tensor, single_image_label)\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.data.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "7f4iVOY01xx9"
      },
      "outputs": [],
      "source": [
        "train_dataset= DatasetFromCSV('trainingdata.csv',16,16,img_transform)\n",
        "test_dataset = DatasetFromCSV(\"testingdata.csv\",16,16,img_transform)\n",
        " \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Va5NIR1xx-"
      },
      "source": [
        "Autoencoder Definition\n",
        "-----------------------\n",
        "We use a convolutional encoder and decoder, which generally gives better performance than fully connected versions that have the same number of parameters.\n",
        "\n",
        "In convolution layers, we increase the channels as we approach the bottleneck, but note that the total number of features still decreases, since the channels increase by a factor of 2 in second convolution, but the spatial size decreases by a factor of 4.\n",
        "\n",
        "Kernel size 4 is used to avoid biasing problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "T9Kk75Kz1xyA",
        "outputId": "c5866368-b7d3-45c4-d476-e3e3377fc89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 101\n",
            "Encoder: Encoder(\n",
            "  (conv1): Conv2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (conv2): Conv2d(1, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 8 x 8\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: 2c x 4 x 4\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), capacity*2, 4, 4) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
        "        return x\n",
        "    \n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon\n",
        "    \n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "autoencoder = autoencoder.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)\n",
        "print(\"Encoder:\", Encoder())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdnl_A2y1xyC"
      },
      "source": [
        "Train Autoencoder\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "vRAmmZE91xyD",
        "outputId": "145b7fbb-586d-4fbc-f43f-4fdabbb5af8c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "Epoch [1 / 50] average reconstruction error: 1.094431\n",
            "Epoch [2 / 50] average reconstruction error: 1.060625\n",
            "Epoch [3 / 50] average reconstruction error: 1.040393\n",
            "Epoch [4 / 50] average reconstruction error: 1.030890\n",
            "Epoch [5 / 50] average reconstruction error: 1.027802\n",
            "Epoch [6 / 50] average reconstruction error: 1.027372\n",
            "Epoch [7 / 50] average reconstruction error: 1.027503\n",
            "Epoch [8 / 50] average reconstruction error: 1.027525\n",
            "Epoch [9 / 50] average reconstruction error: 1.027444\n",
            "Epoch [10 / 50] average reconstruction error: 1.027372\n",
            "Epoch [11 / 50] average reconstruction error: 1.027346\n",
            "Epoch [12 / 50] average reconstruction error: 1.027345\n",
            "Epoch [13 / 50] average reconstruction error: 1.027346\n",
            "Epoch [14 / 50] average reconstruction error: 1.027345\n",
            "Epoch [15 / 50] average reconstruction error: 1.027344\n",
            "Epoch [16 / 50] average reconstruction error: 1.027344\n",
            "Epoch [17 / 50] average reconstruction error: 1.027344\n",
            "Epoch [18 / 50] average reconstruction error: 1.027344\n",
            "Epoch [19 / 50] average reconstruction error: 1.027344\n",
            "Epoch [20 / 50] average reconstruction error: 1.027344\n",
            "Epoch [21 / 50] average reconstruction error: 1.027344\n",
            "Epoch [22 / 50] average reconstruction error: 1.027344\n",
            "Epoch [23 / 50] average reconstruction error: 1.027344\n",
            "Epoch [24 / 50] average reconstruction error: 1.027344\n",
            "Epoch [25 / 50] average reconstruction error: 1.027344\n",
            "Epoch [26 / 50] average reconstruction error: 1.027344\n",
            "Epoch [27 / 50] average reconstruction error: 1.027344\n",
            "Epoch [28 / 50] average reconstruction error: 1.027344\n",
            "Epoch [29 / 50] average reconstruction error: 1.027344\n",
            "Epoch [30 / 50] average reconstruction error: 1.027344\n",
            "Epoch [31 / 50] average reconstruction error: 1.027344\n",
            "Epoch [32 / 50] average reconstruction error: 1.027344\n",
            "Epoch [33 / 50] average reconstruction error: 1.027344\n",
            "Epoch [34 / 50] average reconstruction error: 1.027344\n",
            "Epoch [35 / 50] average reconstruction error: 1.027344\n",
            "Epoch [36 / 50] average reconstruction error: 1.027344\n",
            "Epoch [37 / 50] average reconstruction error: 1.027344\n",
            "Epoch [38 / 50] average reconstruction error: 1.027344\n",
            "Epoch [39 / 50] average reconstruction error: 1.027344\n",
            "Epoch [40 / 50] average reconstruction error: 1.027344\n",
            "Epoch [41 / 50] average reconstruction error: 1.027344\n",
            "Epoch [42 / 50] average reconstruction error: 1.027344\n",
            "Epoch [43 / 50] average reconstruction error: 1.027344\n",
            "Epoch [44 / 50] average reconstruction error: 1.027344\n",
            "Epoch [45 / 50] average reconstruction error: 1.027344\n",
            "Epoch [46 / 50] average reconstruction error: 1.027344\n",
            "Epoch [47 / 50] average reconstruction error: 1.027344\n",
            "Epoch [48 / 50] average reconstruction error: 1.027344\n",
            "Epoch [49 / 50] average reconstruction error: 1.027344\n",
            "Epoch [50 / 50] average reconstruction error: 1.027344\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "autoencoder.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in train_dataloader:\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "        \n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = autoencoder(image_batch)\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = './pretrained/autoencoder_v2_conv.pth'\n",
        "torch.save(autoencoder.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI9L0kot1xyE"
      },
      "source": [
        "Plot Training Curve\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "r11A_7p81xyF",
        "outputId": "32b68496-e168-40dc-db0f-c8f5fda11a7f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOFFB361xyG"
      },
      "source": [
        "Load Pre-Trained Autoencoder\n",
        "-----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "TTwnIKmZ1xyG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing ...\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "if not os.path.isdir('./pretrained'):\n",
        "    os.makedirs('./pretrained')\n",
        "print('importing ...')\n",
        "autoencoder.load_state_dict(torch.load('./pretrained/autoencoder_v2_conv.pth'))\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DFmFEsq1xyH"
      },
      "source": [
        "Evaluate on the Test Set\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "AzcCIA0K1xyI",
        "outputId": "38e9c05c-09b0-4a7f-cdab-e95ebefba6b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average reconstruction error: 1.027344\n"
          ]
        }
      ],
      "source": [
        "# set to evaluation mode\n",
        "autoencoder.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, _ in test_dataloader:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = autoencoder(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "33622b258e849c48cdbc49d77946df62c6974755a1354989ad23976d61ac869e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
